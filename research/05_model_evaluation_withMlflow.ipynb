{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ajay\\\\Desktop\\\\myPortfolio\\\\CommentAnalysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    path_of_model: Path\n",
    "    x_train_file_path: Path\n",
    "    y_train_file_path:Path\n",
    "    testing_data: Path\n",
    "    mlflow_uri: str\n",
    "    root_dir: Path\n",
    "    transformer:Path\n",
    "    file_path:Path\n",
    "    params_file_path:Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CommentAnalysis.constants import *\n",
    "from src.CommentAnalysis.utils.common import read_yaml, create_directories, save_json\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = params_filepath\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        config=self.config.Model_Evaluation\n",
    "        Modelconfig = self.config.prepare_model\n",
    "        dataconfg=self.config.data_Transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=Modelconfig.trained_model_path,\n",
    "            x_train_file_path=dataconfg.x_train_file_path,\n",
    "            y_train_file_path=dataconfg.y_train_file_path,\n",
    "            testing_data=dataconfg.transform_test_file,\n",
    "            mlflow_uri=\"https://dagshub.com/AIwithAj/CommentAnalysis.mlflow\",\n",
    "            params_file_path=self.params,\n",
    "            root_dir=config.root_dir,\n",
    "            transformer=dataconfg.transformer,\n",
    "            file_path=config.file_path\n",
    "        )\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import dagshub\n",
    "\n",
    "from src.CommentAnalysis.utils.common import load_vectorizer, load_bin, read_data\n",
    "from src.CommentAnalysis import logger\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the Evaluation class with config.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def save_model_info(self, run_id: str, model_path: str, file_path: str, metrics: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Save the model run ID, path and optionally metrics to a JSON file.\n",
    "        \"\"\"\n",
    "        model_info = {'run_id': run_id, 'model_path': model_path}\n",
    "        if metrics:\n",
    "            model_info['metrics'] = metrics\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(model_info, file, indent=4)\n",
    "\n",
    "        logger.info(f'Model info and metrics saved to {file_path}')\n",
    "\n",
    "    def evaluate_model(self, model, X, y):\n",
    "        \"\"\"\n",
    "        Evaluate the model and return classification report & confusion matrix.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X)\n",
    "        report = classification_report(y, y_pred, output_dict=True)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        logger.info('Model evaluation completed')\n",
    "        return report, cm\n",
    "\n",
    "    def log_confusion_matrix(self, cm, dataset_name: str):\n",
    "        \"\"\"\n",
    "        Log confusion matrix as an MLflow artifact.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix: {dataset_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        cm_file_path = self.config.root_dir+f'/confusion_matrix_{dataset_name}.png'\n",
    "        plt.savefig(cm_file_path)\n",
    "        mlflow.log_artifact(cm_file_path)\n",
    "        plt.close()\n",
    "        logger.info(f'Confusion matrix logged for {dataset_name}')\n",
    "\n",
    "    def log_metrics(self, report, prefix) -> dict:\n",
    "        \"\"\"\n",
    "        Log metrics from classification report with a prefix.\n",
    "        Returns a dict of the main metrics for JSON file.\n",
    "        \"\"\"\n",
    "        metrics_summary = {}\n",
    "        for label, metrics in report.items():\n",
    "            if label == 'accuracy':\n",
    "                # accuracy is a float\n",
    "                mlflow.log_metric(f\"{prefix}_accuracy\", metrics)\n",
    "                metrics_summary[f\"{prefix}_accuracy\"] = metrics\n",
    "            elif isinstance(metrics, dict):\n",
    "                mlflow.log_metrics({\n",
    "                    f\"{prefix}_{label}_precision\": metrics['precision'],\n",
    "                    f\"{prefix}_{label}_recall\": metrics['recall'],\n",
    "                    f\"{prefix}_{label}_f1-score\": metrics['f1-score']\n",
    "                })\n",
    "                if label == 'weighted avg':\n",
    "                    metrics_summary[f\"{prefix}_precision\"] = metrics['precision']\n",
    "                    metrics_summary[f\"{prefix}_recall\"] = metrics['recall']\n",
    "                    metrics_summary[f\"{prefix}_f1-score\"] = metrics['f1-score']\n",
    "        logger.info(f'Metrics logged for {prefix} -->{metrics_summary}')\n",
    "        return metrics_summary\n",
    "\n",
    "\n",
    "    def log_params_from_yaml(self, yaml_path):\n",
    "        \"\"\"\n",
    "        Load params from params.yaml and log to MLflow.\n",
    "        \"\"\"\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            params = yaml.safe_load(f)\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, dict):\n",
    "                for subkey, subval in value.items():\n",
    "                    mlflow.log_param(f\"{key}_{subkey}\", subval)\n",
    "            else:\n",
    "                mlflow.log_param(key, value)\n",
    "        logger.info(f'Parameters from {yaml_path} logged to MLflow')\n",
    "\n",
    "    def initiate_model_evaluation(self):\n",
    "        \"\"\"\n",
    "        Main function to run model evaluation and log to MLflow.\n",
    "        \"\"\"\n",
    "        dagshub.init(\n",
    "            repo_owner=\"AIwithAj\",\n",
    "            repo_name=\"CommentAnalysis\",\n",
    "            mlflow=True,\n",
    "        )\n",
    "        mlflow.set_experiment('dvc-pipeline-runs')\n",
    "\n",
    "        try:\n",
    "            with mlflow.start_run() as run:\n",
    "                logger.info('MLflow run started.')\n",
    "\n",
    "                # Load artifacts\n",
    "                model = load_bin(self.config.path_of_model)\n",
    "                vectorizer = load_vectorizer(self.config.transformer)\n",
    "                logger.info('Model and vectorizer loaded.')\n",
    "\n",
    "                X_train_tfidf = sparse.load_npz(self.config.x_train_file_path)\n",
    "                y_train = read_data(self.config.y_train_file_path)\n",
    "\n",
    "                test_data = read_data(self.config.testing_data).dropna(subset=['clean_comment', 'category'])\n",
    "                X_test_tfidf = vectorizer.transform(test_data['clean_comment'].values)\n",
    "                y_test = test_data['category'].values\n",
    "\n",
    "                # Signature & example\n",
    "                input_example = pd.DataFrame(\n",
    "                    X_test_tfidf.toarray()[:5],\n",
    "                    columns=vectorizer.get_feature_names_out()\n",
    "                )\n",
    "                signature = infer_signature(input_example, model.predict(X_test_tfidf[:5]))\n",
    "                logger.info('Input example & signature created.')\n",
    "\n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(\n",
    "                    model,\n",
    "                    \"lgbm_model\",\n",
    "                    signature=signature,\n",
    "                    input_example=input_example\n",
    "                )\n",
    "                logger.info('Model logged to MLflow.')\n",
    "\n",
    "                # Log vectorizer & params\n",
    "                mlflow.log_artifact(self.config.transformer)\n",
    "                self.log_params_from_yaml(self.config.params_file_path)\n",
    "\n",
    "                # Evaluate & log on Training Data\n",
    "                train_report, train_cm = self.evaluate_model(model, X_train_tfidf, y_train)\n",
    "                train_metrics = self.log_metrics(train_report, \"train\")\n",
    "                self.log_confusion_matrix(train_cm, \"Train Data\")\n",
    "\n",
    "                # Evaluate & log on Test Data\n",
    "                test_report, test_cm = self.evaluate_model(model, X_test_tfidf, y_test)\n",
    "                test_metrics = self.log_metrics(test_report, \"test\")\n",
    "                self.log_confusion_matrix(test_cm, \"Test Data\")\n",
    "\n",
    "                # Save model info with metrics\n",
    "                model_path = \"lgbm_model\"\n",
    "                metrics_summary = {\n",
    "                    \"train\": train_metrics,\n",
    "                    \"test\": test_metrics\n",
    "                }\n",
    "                self.save_model_info(run.info.run_id, model_path, self.config.file_path, metrics=metrics_summary)\n",
    "                logger.info(f\"loggig and saving evaluation metrics successfully--{ self.config.file_path}\")\n",
    "                # MLflow tags\n",
    "                mlflow.set_tag(\"model_type\", \"LightGBM\")\n",
    "                mlflow.set_tag(\"task\", \"Sentiment Analysis\")\n",
    "                mlflow.set_tag(\"dataset\", \"YouTube Comments\")\n",
    "\n",
    "                logger.info(\"Model evaluation and logging completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to complete model evaluation: {e}\")\n",
    "            print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:43:31,180: INFO: common: YAML file loaded successfully: config\\config.yaml]\n",
      "[2025-07-10 21:43:31,180: INFO: common: Created directory: artifacts]\n",
      "[2025-07-10 21:43:31,188: INFO: common: Created directory: artifacts/Model_Evaluation]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:43:32,870: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/AIwithAj/CommentAnalysis \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"AIwithAj/CommentAnalysis\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"AIwithAj/CommentAnalysis\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:43:32,895: INFO: helpers: Initialized MLflow to track repo \"AIwithAj/CommentAnalysis\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository AIwithAj/CommentAnalysis initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository AIwithAj/CommentAnalysis initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:43:32,900: INFO: helpers: Repository AIwithAj/CommentAnalysis initialized!]\n",
      "[2025-07-10 21:43:33,041: WARNING: connectionpool: Retrying (Retry(total=4, connect=5, read=4, redirect=5, status=5)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /AIwithAj/CommentAnalysis.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=dvc-pipeline-runs]\n",
      "[2025-07-10 21:43:34,224: INFO: 1966844431: MLflow run started.]\n",
      "[2025-07-10 21:43:34,470: INFO: common: Binary file loaded: artifacts/prepare_base_model/trained_model.pkl]\n",
      "[2025-07-10 21:43:34,636: INFO: 1966844431: Model and vectorizer loaded.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajay\\Desktop\\myPortfolio\\CommentAnalysis\\.env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:43:37,487: INFO: 1966844431: Input example & signature created.]\n",
      "[2025-07-10 21:45:06,193: INFO: 1966844431: Model logged to MLflow.]\n",
      "[2025-07-10 21:45:10,978: INFO: 1966844431: Parameters from params.yaml logged to MLflow]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajay\\Desktop\\myPortfolio\\CommentAnalysis\\.env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:45:15,500: INFO: 1966844431: Model evaluation completed]\n",
      "[2025-07-10 21:45:18,153: INFO: 1966844431: Metrics logged for train--->{'train_precision': 0.9356483185453919, 'train_recall': 0.9323282969344147, 'train_f1-score': 0.9324334295052213}]\n",
      "[2025-07-10 21:45:19,114: INFO: 1966844431: Confusion matrix logged for Train Data]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajay\\Desktop\\myPortfolio\\CommentAnalysis\\.env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-10 21:45:20,407: INFO: 1966844431: Model evaluation completed]\n",
      "[2025-07-10 21:45:23,116: INFO: 1966844431: Metrics logged for test--->{'test_precision': 0.8634689945082465, 'test_recall': 0.8624338624338624, 'test_f1-score': 0.8614695631272568}]\n",
      "[2025-07-10 21:45:24,407: INFO: 1966844431: Confusion matrix logged for Test Data]\n",
      "[2025-07-10 21:45:24,416: INFO: 1966844431: Model info and metrics saved to artifacts/Model_Evaluation/experiment_info.json]\n",
      "[2025-07-10 21:45:24,425: INFO: 1966844431: loggig and saving evaluation metrics successfully--artifacts/Model_Evaluation/experiment_info.json]\n",
      "[2025-07-10 21:45:25,976: INFO: 1966844431: Model evaluation and logging completed successfully.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    evaluation.initiate_model_evaluation()\n",
    "    # evaluation.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "   raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(r\"C:\\Users\\ajay\\Desktop\\myPortfolio\\CommentAnalysis\\artifacts\\data_Transformation\\test\\transform_test_file.csv\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
